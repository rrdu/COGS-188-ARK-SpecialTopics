{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Special Topics Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Rebecca Du\n",
    "- Alexander Tang\n",
    "- Kevin Su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic and outline\n",
    "\n",
    "### Main topic\n",
    "Our main goal is to cover the basics of natural language processing. We will discuss the definition and basic structure of language models (including grammar, parsing, and word embeddings). We will also briefly cover the different types of different models (such as bag-of-word, n-gram, and HMM).\n",
    "\n",
    "We then plan on covering how LLMs work by doing a live demo using Langchain, OpenAI’s API, and Pinecone. In the live demo, we will use a pre-embedded dataset from Pinecone, which will help simplify things so that we can cover the basics more in depth. We will then walk through how to use OpenAI’s API to perform a similarity search on the embeddings to produce a Q&A chatbot trained on the pre-embedded dataset.\n",
    "\n",
    "\n",
    "### Learning goal for students \n",
    "Students will understand the basic concept of how natural language processing works, the various models that can perform the task, and how to implement those basic principles into a simple LLM Q&A chatbot.\n",
    "\n",
    "### Outline of the topic\n",
    "- NLP Basics\n",
    "    - Purpose of NLP (communication, learning, advancement)\n",
    "    - **Language model definition + types**\n",
    "        - Bag-of-words (brief recap)\n",
    "        - N-gram\n",
    "        - HMM\n",
    "    - Grammar\n",
    "    - Parsing\n",
    "    - PCFG\n",
    "        - Illustrated through an example\n",
    "    - **Word embeddings**\n",
    "        - How to represent word relations to one another\n",
    "        - Visualization of word embeddings using graph\n",
    "    - Complications\n",
    "        - How to deal with ambiguity, time/tense, metaphors, metonymy, etc.\n",
    "- Live code demo: implementing a simple Q&A LLM chatbot\n",
    "    - **How to use Langchain**\n",
    "        - First go over what Langchain is: it’s a framework for developing applications powered by LLMs\n",
    "    - How to use Pinecone as a vector database\n",
    "        - First go over what Pinecone is: it’s used to store word embedding vectors\n",
    "    - How to implement OpenAI’s API\n",
    "        - First go over what an API is\n",
    "        - Then go over how to implement the API using Langchain\n",
    "        - Lastly go over how to connect the API to the Pinecone vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedagogy\n",
    "\n",
    "\n",
    "### Readings\n",
    "1) **Russell & Norvig (Ch. 23): Natural Language Processing**: (pgs 1514-1536)\n",
    "    - Chapter 23 discusses in detail what natural language processing (NLP) is. This is the most important chapter that students should read to understand what will be covered in the lecture. The chapter is divided into 3 main sections: language models, grammar and parsing, and complications of NLP that may arise during tasks.\n",
    "    - The first section of the chapter (23.1 and all its subsections) delves into what language models are and the various types that exist. It begins by defining what a language model is before describing different examples of language models in detail. This section covers models like the bag-of-words model, n-gram word models, and generative models before comparing them based on the strengths and weaknesses of each one. Students will also be introduced to word representation and categorization via methods such as word embedding (further nuanced in Chapter 24.1) and part-of-speech (POS) tagging.\n",
    "    - The second part of the assigned readings (23.2-23.4) discusses grammar and parsing. Students will learn how grammar defines what phrases are allowed and how it can be implemented algorithmically. Students will briefly learn about parsing. Due to the complicated nature of Chomsky Normal Form and the CYK algorithm, we will omit this part of the section from the reading. \n",
    "    - The remainder of Chapter 23 discusses complications and challenges that NLP may struggle with. Students will not need to read these as they will be covered in class and are rather intuitive. \n",
    "    - After reading Chapter 23, students are expected to understand and explain what language models, grammar, and parsing are.\n",
    "\n",
    "\n",
    "2) **Russell & Norvig (Ch. 24.1): Word Embeddings**: \n",
    "    - Given that the bulk of the lecture portion discusses the algorithmic aspect of NLP as described in Chapter 23, the only part that students should read from Chapter 24 is section 24.1, which covers word embeddings. The section provides a simple explanation of what word embeddings are and also provides a good visual representation of why they are important. This will help foster an intuitive understanding of what word embeddings are. After reading this section, students should know what word embeddings are and be able to create and explain a brief example of how they work (such as the country to capital analogy described in the section).\n",
    "    \n",
    "3) **[Langchain Quickstart Documentation](https://python.langchain.com/docs/get_started/quickstart/)**: \n",
    "    - This is a simple guide to ensure that students are aware of the general steps they need to take to get started with Langchain and will provide helpful context for the demo. \n",
    "\n",
    "### Lecture description\n",
    "The lecture will follow the general structure of Chapter 23 and Chapter 24.1 that is outlined above in the readings section. We will begin with the lecture by covering the basics of NLP. We will first discuss what NLP is by providing some context as to why computers should learn it, and then defining what it is. Then, we will briefly recap what bag-of-words is before moving onto language models that students might not be as familiar with: n-gram and HMM. Each of these models will be explained thoroughly through an example. Each example will also be followed by a general summary slide with key information students should remember about the models. This will take about 15 minutes to complete. Rebecca will cover this portion. \n",
    "\n",
    "Next, we will discuss grammar and parsing. We will briefly define the terms then delve deeper into probabilistic context-free grammar (PCFG) through an illustrative example. Since PCFG is somewhat dense and we want to ensure that students understand the concept thoroughly, this should take about 5 to 10 minutes, although the content is only 2 slides long. Rebecca will also cover this portion. \n",
    "\n",
    "Finally, we will discuss word embeddings and some challenges in language processing that NLP models might face. We will begin by explaining in simple terms what word embeddings are, before transitioning to 2 key interactive examples that involve students answering questions to further elaborate on what word embeddings can do. Then, we will briefly discuss some ways that the models can deal with issues such as ambiguity and metaphors, but it is expected that students will know the general idea of this part due to having completed the readings ahead of time. Kevin and Rebecca will work together on this portion, which should take about 15-20 minutesl \n",
    "\n",
    "The main takeaways that students should get from the lecture portion are a solid understanding of what language models, PCFG, and word embeddings are. For instance, they should be able to explain what the GloVe embedding vector example is about. We plan on the traditional lecture section taking up roughly 30 minutes. This section will be covered by Kevin and Rebecca. \n",
    "\n",
    "Then we will transition into a live demo code where we will implement an LLM application through Pinecone (vector database), OpenAI’s API (LLM model), and Langchain (Python LLM library). We will demonstrate how these 3 components interact with each other and how to implement each section of the Q&A chatbot. We plan on this section being no longer than 20 minutes. Alexander will cover the demo. \n",
    "\n",
    "\n",
    "### Active learning\n",
    "- Our active learning exercise will be a live coding demo to introduce how LLMs are actually implemented into a real life application.\n",
    "    - We will be following [this tutorial](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-agent.ipynb#scrollTo=qNyRsz0ZXXaq) posted by the developers of Langchain. \n",
    "    - The example includes using a pre-embedded dataset from Pinecone, using OpenAI’s API as a LLM model, and Langchain as an LLM framework to implement all the sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates\n",
    "- Monday 5/20\n",
    "- Monday 5/27\n",
    "- Monday 5/29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will communicate through discord when discussing the project\n",
    "- We will split all the work up evenly between the 3 members \n",
    "- We will consult with all 3 members before making any big decisions\n",
    "- When faced with any potential conflict or difficulty, we will communicate our thoughts and feelings with all team members and try to resolve the conflict as smoothly as possible. \n",
    "    - If necessary, we will ask Professor Fleischer if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| Thursday 5/2  |  N/A |  Submit Project Proposal  | N/A | \n",
    "| Proposal Feedback Released |  N/A |  All: Begin slides/demo preparation | Look over feedback, adjust proposal and materials to fit | \n",
    "| Friday 5/10  | 11 AM - 12 PM  | All: Continue slides/demos  | Those that worked on the demo will look over/edit slides and vice versa  |\n",
    "| Friday 5/17  | 11 AM - 12 PM  | All: Practice slides/demo individually | All: Rehearse slides/demo virtually or in-person   |\n",
    "| Presentation Day (TBD)  | 3 - 4 PM  | All: Rehearse slides/demo | Present!  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
