{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 188 - Special Topics Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Rebecca Du\n",
    "- Alexander Tang\n",
    "- Kevin Su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic and outline\n",
    "\n",
    "### Main topic\n",
    "Our main goal is to cover the basics of natural language processing. We will discuss the definition and basic structure of language models (including word embeddings, phrase structure grammars, etc.). We will also briefly cover the different types of different models (such as n-gram and transformer).\n",
    "\n",
    "We then plan on covering how LLMs work by doing a live demo using Langchain, OpenAI’s API, and Pinecone. In the live demo, we will use a pre-embedded dataset from Pinecone, which will help simplify things so that we can cover the basics more in depth. We will then walk through how to use OpenAI’s API to perform a similarity search on the embeddings to produce a Q&A chatbot trained on the pre-embedded dataset.\n",
    "\n",
    "\n",
    "### Learning goal for students \n",
    "Students will understand the basic concept of how natural language processing works, the various models that can perform the task, and how to implement those basic principles into a simple LLM Q&A chatbot.\n",
    "\n",
    "### Outline of the topic\n",
    "- NLP Basics\n",
    "    - Purpose of NLP (communication, learning, advancement)\n",
    "    - **Language model definition + types** (n-gram, transformer models, etc.)\n",
    "    - Grammar\n",
    "    - Parsing and algorithms\n",
    "        - **CYK algorithm**\n",
    "        - **Chomsky Normal Form grammar**\n",
    "    - **Word embeddings**\n",
    "        - How to represent word relations to one another\n",
    "        - Visualization of word embeddings using graph\n",
    "    - Complications\n",
    "        - How to deal with ambiguity, time/tense, metaphors, metonymy, etc.\n",
    "- Live code demo: implementing a simple Q&A LLM chatbot\n",
    "    - **How to use Langchain**\n",
    "        - First go over what Langchain is: it’s a framework for developing applications powered by LLMs\n",
    "    - How to use Pinecone as a vector database\n",
    "        - First go over what Pinecone is: it’s used to store word embedding vectors\n",
    "    - How to implement OpenAI’s API\n",
    "        - First go over what an API is\n",
    "        - Then go over how to implement the API using Langchain\n",
    "        - Lastly go over how to connect the API to the Pinecone vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pedagogy\n",
    "\n",
    "\n",
    "### Readings\n",
    "1) **Russell & Norvig (Ch. 23): Natural Language Processing**: \n",
    "    - Chapter 23 discusses in detail what natural language processing (NLP) is. This is the most important chapter that students should read to understand what will be covered in the lecture. The chapter is divided into 3 main sections: language models, grammar and parsing, and complications of NLP that may arise during tasks.\n",
    "    - The first section of the chapter (23.1 and all its subsections) delves into what language models are and the various types that exist. It begins by defining what a language model is before describing different examples of language models in detail. This section covers models like the bag-of-words model, n-gram word models, and generative models before comparing them based on the strengths and weaknesses of each one. Students will also be introduced to word representation and categorization via methods such as word embedding (further nuanced in Chapter 24.1) and part-of-speech (POS) tagging.\n",
    "    - The second section of the chapter (23.2-23.4) discusses grammar and parsing. Students will learn how grammar defines what phrases are allowed and how it can be implemented algorithmically. Additionally, students will learn about the various types of parsing (deterministic, shift-reduce, dependency, etc.), Chomsky Normal Form grammar, and the CYK algorithm for determining whether a given string can be generated by a specific grammar and how.\n",
    "    - The third section of the chapter (23.5-23.6) discusses complications that can arise when performing various NLP tasks. These sections address how to deal with parts of language that we don’t think twice about when comprehending, but might pose a challenge to the NLP algorithm. This includes things such as quantification, time and tense, ambiguity, metaphors, and metonymy. \n",
    "    - After reading Chapter 23, students are expected to understand and explain what language models, grammar, and parsing are and be able to give brief examples and explanations of each (e.g. describing an n-gram model, explaining the structure and use of the CYK algorithm, etc.).\n",
    "\n",
    "2) **Russell & Norvig (Ch. 24.1): Word Embeddings**: \n",
    "    - Given that the bulk of the lecture portion discusses the algorithmic aspect of NLP as described in Chapter 23, the only part that students should read from Chapter 24 is section 24.1, which covers word embeddings. The section provides a simple explanation of what word embeddings are and also provides a good visual representation of why they are important. This will help foster an intuitive understanding of what word embeddings are. After reading this section, students should know what word embeddings are and be able to create and explain a brief example of how they work (such as the country to capital analogy described in the section).\n",
    "    \n",
    "3) **[Langchain Quickstart Documentation](https://python.langchain.com/docs/get_started/quickstart/)**: \n",
    "    - This is a simple guide to ensure that students are aware of the general steps they need to take to get started with Langchain and will provide helpful context for the demo. \n",
    "\n",
    "### Lecture description\n",
    "The lecture will follow the general structure of Chapter 23 and Chapter 24.1 that is outlined above in the readings section. We will begin with the lecture by covering the basics of NLP. We will first discuss what NLP is by providing some context as to why computers should learn it, and then defining what it is. Then, we will briefly discuss the distinction between various NLP models such as bag-of-words, n-gram, and generative models before summarizing it in a table that will be on the slides. We will also explain what word embedding is through an example of embedding vectors computed by the GloVe algorithm. This will take about 10 minutes. \n",
    "\n",
    "Next, we will discuss grammar and parsing. We will briefly define the two terms (including an overview of the different types of parsing via a summary table), before going into more detail about the CYK algorithm and Chomsky Normal Form grammar. This will take about 12 minutes.\n",
    "\n",
    "Finally, we will cover some challenges in language processing that NLP models might face. We will briefly discuss some ways that the models can deal with issues such as ambiguity and metaphors, but it is expected that students will know the general idea of this part due to having completed the readings ahead of time. This will be brief and only take about 8 minutes.\n",
    "\n",
    "The main takeaways that students should get from the lecture portion are a solid understanding of what NLP models, word embeddings, and the CYK algorithm/Chomsky Normal Form are. For instance, they should be able to explain what the GloVe embedding vector example is about. We plan on the traditional lecture section taking up roughly 30 minutes. We will also split this section up between the 3 of us.\n",
    "\n",
    "Then we will transition into a live demo code where we will implement an LLM application through Pinecone (vector database), OpenAI’s API (LLM model), and Langchain (Python LLM library). We will demonstrate how these 3 components interact with each other and how to implement each section of the Q&A chatbot. We plan on this section being no longer than 20 minutes. We will also split this section up between the 3 of us.\n",
    "\n",
    "\n",
    "### Active learning\n",
    "- Our active learning exercise will be a live coding demo to introduce how LLMs are actually implemented into a real life application.\n",
    "    - We will be following [this tutorial](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-agent.ipynb#scrollTo=qNyRsz0ZXXaq) posted by the developers of Langchain. \n",
    "    - The example includes using a pre-embedded dataset from Pinecone, using OpenAI’s API as a LLM model, and Langchain as an LLM framework to implement all the sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dates\n",
    "- Monday 5/20\n",
    "- Monday 5/27\n",
    "- Monday 5/29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will communicate through discord when discussing the project\n",
    "- We will split all the work up evenly between the 3 members \n",
    "- We will consult with all 3 members before making any big decisions\n",
    "- When faced with any potential conflict or difficulty, we will communicate our thoughts and feelings with all team members and try to resolve the conflict as smoothly as possible. \n",
    "    - If necessary, we will ask Professor Fleischer if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| Thursday 5/2  |  N/A |  Submit Project Proposal  | N/A | \n",
    "| Proposal Feedback Released |  N/A |  All: Begin slides/demo preparation | Look over feedback, adjust proposal and materials to fit | \n",
    "| Friday 5/10  | 11 AM - 12 PM  | All: Continue slides/demos  | Those that worked on the demo will look over/edit slides and vice versa  |\n",
    "| Friday 5/17  | 11 AM - 12 PM  | All: Practice slides/demo individually | All: Rehearse slides/demo virtually or in-person   |\n",
    "| Presentation Day (TBD)  | 3 - 4 PM  | All: Rehearse slides/demo | Present!  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
